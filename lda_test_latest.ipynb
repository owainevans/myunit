{
 "metadata": {
  "name": "lda_test"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "from venture.venturemagics.ip_parallel import *\nfrom venture.venturemagics.reg_demo_utils import *\nimport time\ndef multi(dist): return np.argmax(np.random.multinomial(1,dist))\nfrom scipy.stats import itemfreq\ndef normalize(lst):\n    s=float(sum(lst))\n    return [el/s for el in lst] if s>0 else 'negative sum'\nro = lambda ar: np.round(ar,2)\n\n\ndef mk_lda(clda=True,no_topics=5,size_vocab=20,\n           alpha_t_prior='(gamma 1 1)',alpha_w_prior='(gamma 1 1)'):\n    lda='''\n    [assume topics %i]\n    [assume vocab %i]\n    [assume alpha_t %s ]\n    [assume alpha_w %s ]\n    [assume doc (mem (lambda (doc_ind) (make_sym_dir_mult alpha_t topics) ) )]\n    [assume topic (mem (lambda (topic_ind) (make_sym_dir_mult alpha_w vocab) ) )]\n    [assume word (mem (lambda (doc_ind word_ind) ( (topic ((doc doc_ind)) ) )  ) ) ]\n    ''' % (no_topics,size_vocab,alpha_t_prior,alpha_w_prior)\n\n    ulda='''\n    [assume topics %i]\n    [assume vocab %i]\n    [assume alpha_t %s ]\n    [assume alpha_w %s ]\n    [assume doc (mem (lambda (doc_ind) (symmetric_dirichlet alpha_t topics)))]\n    [assume topic (mem (lambda (topic_ind) (symmetric_dirichlet alpha_w vocab) ) )]\n    [assume z (mem (lambda (doc_ind word_ind) (categorical (doc doc_ind)) ) ) ]\n    [assume w_th (lambda (doc_ind word_ind) (categorical (topic (z doc_ind word_ind) ) ) ) ]\n    \n    [assume word (mem (lambda (doc_ind word_ind)\n      (categorical (topic (z doc_ind word_ind) ) ) ) ) ] \n    ''' % (no_topics,size_vocab,alpha_t_prior,alpha_w_prior)\n    return lda if clda else ulda\n\ndef test_lda():\n    def gen_data(lite=0,clda=1):\n        vs=[mk_p_ripl() for i in range(2)] if not(lite) else [mk_l_ripl() for i in range(2)]\n        no_topics=2;size_vocab=20; nt=no_topics; sv=size_vocab\n        vs[0].execute_program(mk_lda(clda=clda,no_topics=nt,size_vocab=sv,\n                                     alpha_w_prior='.5') )\n        vs[1].execute_program(mk_lda(clda=clda, no_topics=nt,size_vocab=sv,\n                                     alpha_w_prior='.001') )\n        no_docs = 5; words_per_doc = 300; corpus={}\n        for i,v in enumerate(vs):\n            corpus[i]=[]\n            for doc in range(no_docs):\n                p=[v.predict('(word %i %i)' % (doc,ind) ) for ind in range(words_per_doc)]\n                corpus[i].append(p)\n        \n        assert 3>np.mean(np.abs((np.mean(corpus[0],axis=1) - np.mean(range(sv))))) \n        assert sum(np.std(corpus[1],axis=1)) < sum(np.std(corpus[0],axis=1))\n        \n        if not(clda):\n            uni = np.array( vs[0].sample('(topic 0)') )\n            concen = np.array( vs[1].sample('(topic 0)') )\n            \n            assert .1 > np.mean( np.abs( uni - 1./size_vocab) )\n            assert any(.85 < np.abs( concen - 1./size_vocab) )\n             \n            print [v.sample('(topic 0)') for v in vs]\n            print [v.sample('(doc 0)') for v in vs]\n    \n    gen_data(lite=0,clda=1)\n    gen_data(lite=0,clda=0)\n    gen_data(lite=1,clda=1)\n    return None\n\ndef no_x(n,x): return str(int(n)).count(str(int(x)))\n\ndef gen_zdocs(half_doc_length=20,no_docs=5,size_vocab=50,no_topics=2,easy=0):\n    N=half_doc_length\n    topics=[]\n    for k in range(no_topics):\n        l=[1.+np.mod(i+k,size_vocab)**6 for i in range(1,size_vocab+1)]\n        topics.append( normalize(np.array(l)**(-1)) )\n        \n    #topics[1]= normalize([1./abs(((size_vocab+1)-i)) for i in range(1,size_vocab+1)])\n    docs=[]; docs_topics=[]\n    for doc in range(no_docs):\n        s,t =  np.random.randint(0,no_topics,2)\n        words = [multi(topics[s]) for rep in range(N)] + [multi(topics[t]) for rep in range(N)]\n        docs.append( words )\n        vec = np.zeros(no_topics); vec[s]+=.5; vec[t]+=.5\n        docs_topics.append( vec )\n    \n    if easy: \n        topics=map(normalize,( [10,10] + [1]*(size_vocab-2), [1]*(size_vocab-2) + [10,10] ) ) \n        docs = [ [0,1]*N, [size_vocab,size_vocab-1]*N ] * no_docs\n        docs_topics = []\n    \n    out = {'topics':topics,'docs':docs,'docs_topics':docs_topics}\n    print 'docs sample of doc0: ',out['docs'][0][:10]\n    print 'docs sample of doc1: ',out['docs'][1][:10]\n    return out\n\ndef plot_disc(xs,clist=[],title=None):\n    #if not xs: xs=np.arange(len(clist[0]))\n    fig, ax = plt.subplots(len(clist),1,figsize=(15,3*len(clist)))\n    width=0.4\n    for i,c in enumerate(clist):\n        xs=range(len(c))\n        ax[i].bar(xs,c, width,label=str(i))\n        ax[i].set_xticks(xs)\n        ax[i].set_xticklabels( map(str,xs) )\n        ax[i].legend()\n        if title: ax[i].set_title(title)\n    fig.tight_layout()\n    return fig\n\ndef plot_docs(dlst,no_bins=20):\n    fig,ax = plt.subplots(1,2,figsize=(14,4))\n    xr=range(min(if_lst_flatten(dlst)),max(if_lst_flatten(dlst)))\n    counts = [np.histogram(d,bins=xr,density=True) for d in dlst]\n    [ax[0].hist(d,bins=no_bins,alpha=0.3,label=str(i)) for i,d in enumerate(dlst)]\n    [ax[1].bar(xr,counts[i],width,label=str(i)) for i,d in enumerate(dlst)]\n    [ax[i].legend() for i in [0,1]]\n    return fig\n\ndef inf_til(v,n=2000,t=120):\n    st=time.time(); el = 0; total_steps = 0\n    while el<t:\n        v.infer(n); print 'log: ', v.get_global_logscore()\n        el = time.time() - st\n        total_steps += n\n    return total_steps\n        \n        ",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "no_topics=2;size_vocab=40; nt=no_topics; sv=size_vocab\nno_docs = 6; half_doc_length=30\nclda=False\nv=mk_p_ripl()\n#v=MRipl(32)\n# note, model thinks more topics\nv.execute_program(mk_lda(clda=False,no_topics=nt,size_vocab=sv,\n                     alpha_t_prior='(gamma 1 10)',alpha_w_prior='(gamma 1 10)'))\n\nnp.random.seed(1)\nout = gen_zdocs(half_doc_length=half_doc_length,\n                no_docs=no_docs,size_vocab=sv,no_topics=nt,easy=0)\ntopics = out['topics']; docs =  out['docs']; docs_topics = out['docs_topics']\n\n# [v.observe('(word %i %i)'%(i,j),'1.') for i in range(5) for j in range(5)]\n\nfor doc_ind,doc in enumerate(docs):\n    for word_ind in range(len(doc)):\n        v.observe('(word %i %i)'%(doc_ind,word_ind),'atom<%i>'%doc[word_ind])\nloops=3\nsteps=6*10**3\nprint 'perloop: %i, total steps: %i'% (steps,steps*loops)\nst= time.time()\n\nfor repeats in range(loops):\n    #total_steps = inf_til(v,n=steps,t=20)\n    \n    print 'log: ', v.get_global_logscore()\n    \n    \n    inf_topics= [v.sample('(topic %i)'%i) for i in range(no_topics)]\n    inf_docs = [v.sample('(doc %i)'%i) for i in range(no_docs)]\n    #print 'true topics: ', ro(topics); print 'inf_topics: ', ro(inf_topics)\n    #print 'true docs ', ro(docs); print 'inf_docs', ro(inf_docs)\n    \n    doc_wordhists = [normalize(np.bincount(doc,minlength=size_vocab)) for doc in docs]\n    \n    if repeats==0:\n        true_wordhists=[]\n        for doc_i in range(no_docs):\n            doc_emp = [v.sample('(w_th %i 1000)'%doc_i) for reps in range(300)]\n            doc_emp_norm = normalize(np.bincount(doc_emp,minlength=size_vocab))\n            \n            doc_simplex = v.sample('(doc %i)'%doc_i)\n            word_hist = np.sum([ docs_topics[doc_i][topic_ind] * np.array(topics[topic_ind]) for topic_ind in range(no_topics)],\n                               axis=0)\n            assert .05 > abs(1-sum(word_hist))\n            true_wordhists.append(word_hist)\n    \n    #print 'computed wordhists',map(ro,(true_wordhists))\n    #print 'true empirical wordhists',map(ro,doc_wordhists)\n    emp_diffs = [np.abs(d-t) for d,t in zip(doc_wordhists,true_wordhists) ]\n    #print 'diff of computed wordhists and true empirical wordhists:', map(ro,emp_diffs)\n    print 'mean of all diffs:',np.mean(emp_diffs)\n    \n    \n    inf_wordhists=[]\n    for doc_i in range(no_docs):\n        doc_emp = [v.sample('(w_th %i 1000)'%doc_i) for reps in range(300)]\n        doc_emp_norm = normalize(np.bincount(doc_emp,minlength=size_vocab))\n        \n        doc_simplex = v.sample('(doc %i)'%doc_i)\n        word_hist = np.sum([ doc_simplex[topic_ind] * np.array(inf_topics[topic_ind]) for topic_ind in range(no_topics)],\n                           axis=0)\n        assert .05 > abs(1-sum(word_hist))\n        inf_wordhists.append(word_hist)\n    \n    #print 'doc_wordhist: ', ro(doc_wordhists)\n    #print 'inf_wordhist: ', ro(inf_wordhists)\n    assert all( [len(a)==len(b) for a,b in zip(doc_wordhists,inf_wordhists)] ) \n    diffs = [ np.abs(doc_wh - inf_wh) for doc_wh,inf_wh in zip(doc_wordhists,inf_wordhists) ]\n    #ratios = [ max(d,s)/min(d,s) for d,s in zip(doc_wordhists,inf_wordhists) ]\n    diffs_inf_uni = [ np.abs((size_vocab**-1)*np.ones(size_vocab) - inf_wh) for doc_wh,inf_wh in zip(doc_wordhists,inf_wordhists) ]\n    diffs_docs_uni = [ np.abs((size_vocab**-1)*np.ones(size_vocab) - doc_wh) for doc_wh,inf_wh in zip(doc_wordhists,inf_wordhists) ]\n    print 'diffs between inf_word_hists and doc_wordhists: ', map(ro,diffs)[0]\n    print 'mean max diff: %.3f %.3f'%(np.mean(diffs),np.max(diffs))\n    #print 'ratio mean max',np.mean(ratios),np.max(ratios)\n    print 'mean max diffs_inf_uni: %.3f %.3f'%(np.mean(diffs_inf_uni),np.max(diffs_inf_uni))\n    print 'mean max diffs_docs_uni:%.3f %.3f'%(np.mean(diffs_docs_uni),np.max(diffs_docs_uni))\n    if repeats==0: print 'loop time: ', time.time() - st\n    \n    v.infer(steps)\nprint 'total time: ',time.time() - st\n",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "docs sample of doc0:  [39, 39, 39, 38, 39, 38, 38, 38, 39, 38]\ndocs sample of doc1:  [39, 39, 38, 38, 38, 38, 38, 38, 38, 39]\nperloop: 6000, total steps: 18000"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "\nlog:  -121.143100212\nmean of all diffs:"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": " 0.00286385724536\ndiffs between inf_word_hists and doc_wordhists: "
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": " [ 0.02  0.    0.01  0.14  0.1   0.    0.    0.    0.    0.    0.    0.    0.\n  0.03  0.    0.    0.    0.02  0.    0.    0.    0.19  0.    0.19  0.    0.\n  0.    0.    0.    0.    0.    0.09  0.    0.    0.15  0.    0.    0.05\n  0.58  0.38]\nmean max diff: 0.047 0.657\nmean max diffs_inf_uni: 0.035 0.303\nmean max diffs_docs_uni:0.046 0.658\nloop time:  21.3773510456\nlog: "
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": " -725.129353032\nmean of all diffs: 0.00286385724536\ndiffs between inf_word_hists and doc_wordhists: "
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": " [ 0.02  0.02  0.    0.02  0.01  0.47  0.    0.    0.03  0.    0.    0.\n  0.01  0.    0.01  0.    0.    0.06  0.    0.    0.    0.02  0.    0.    0.\n  0.    0.03  0.    0.    0.    0.1   0.    0.01  0.    0.    0.    0.\n  0.02  0.48  0.32]\nmean max diff: 0.046 0.650\nmean max diffs_inf_uni: 0.035 0.467\nmean max diffs_docs_uni:0.046 0.658\nlog: "
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": " -484.360231161\nmean of all diffs: 0.00286385724536\ndiffs between inf_word_hists and doc_wordhists: "
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": " [ 0.    0.    0.06  0.    0.03  0.    0.01  0.    0.    0.    0.    0.    0.\n  0.    0.    0.    0.    0.    0.    0.    0.06  0.    0.    0.    0.    0.\n  0.    0.    0.21  0.    0.    0.05  0.01  0.05  0.    0.    0.1   0.37\n  0.58  0.4 ]\nmean max diff: 0.048 0.673\nmean max diffs_inf_uni: 0.035 0.346\nmean max diffs_docs_uni:0.046 0.658\ntotal time: "
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": " 60.4788599014\n"
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "with gamma 11 for each\ndocs sample of doc0:  [18, 18, 18, 19, 18, 18, 18, 18, 19, 18]\ndocs sample of doc1:  [0, 19, 0, 19, 19, 19, 19, 19, 19, 0]\nperloop: 20000, total steps: 120000\nlog:  -750.22394929\nmean max diff: 0.0790756525811 0.615721105253\nlog:  -677.109739567\nmean max diff: 0.0805712428138 0.634985518394\nlog:  -616.636493267\nmean max diff: 0.0812569661414 0.603037118991\nlog:  -571.994766636\nmean max diff: 0.0810591927002 0.617981827871\nlog:  -577.04352186\nmean max diff: 0.0827804834784 0.658991933724\nlog:  -540.306825\nmean max diff: 0.0815481546756 0.641704323974\nloop time:  90.8177750111\ntotal time:  108.379724026\n",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "",
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}